{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Directories and file paths\n",
    "mriqc_failed_dir = \"ds004636/derivatives/mriqc_failed\"\n",
    "mriqc_passed_dir = \"ds004636/derivatives/mriqc_passed\"\n",
    "\n",
    "csv_file = \"quality_metrics/BOLD_quality_metrics.csv\"\n",
    "mriqc_passed_txt = \"metadata/mriqc_passed/mriqc_passed_fullname.txt\"\n",
    "mriqc_failed_txt = \"metadata/mriqc_failed/mriqc_failed_fullname.txt\"\n",
    "\n",
    "# Task names to filter\n",
    "TASKS = {\"ANT\", \"CCTHot\", \"WATT3\", \"stopSignal\", \"twoByTwo\", \"DPX\", \"discountFix\", \"motorSelectiveStop\", \"stroop\", \"surveyMedley\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "        # takes in file names from mriqc_failed.txt or mriqc_passed.txt\n",
    "        # and returns subject and task\n",
    "\n",
    "    Args:\n",
    "        filename (string): _description_\n",
    "\n",
    "    Returns:\n",
    "        subject (string)\n",
    "        task (str)\n",
    "    \"\"\"\n",
    "    parts = filename.split(\"_\")\n",
    "    subject = parts[0].replace(\"sub-\", \"\")\n",
    "    task = next((t for t in TASKS if t in filename), None)\n",
    "    return subject, task\n",
    "\n",
    "\n",
    "def extract_metrics(html_path, metrics):\n",
    "    \"\"\"given the full html name, and metrics (found in BOLD_quality_metrics.csv)\n",
    "    extract relevant information from the html files, populates the data dictionary with \n",
    "    key(metric) and value(metric value) pairs\n",
    "\n",
    "    Args:\n",
    "        html_path (_type_): _description_\n",
    "        metrics (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        data: a dictionary with keys metric names and values: metric values e\n",
    "    \"\"\"\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    # relevant metric key and values are under \"other\" on html files\n",
    "    other_section = soup.find(id=\"other\")\n",
    "    if not other_section:\n",
    "        print(f\" other table not found for {html_path}\")\n",
    "        return data  # Return empty if \"Other\" section is not found\n",
    "    \n",
    "    # in the other section, get the table with id \"iqms-table\" which is where ketric key-value pairs are listed\n",
    "    table = other_section.find_next(\"table\", {\"id\": \"iqms-table\"})\n",
    "    if not table:\n",
    "        print(f\"iqms table not found for {html_path}\")\n",
    "        return data  # Return empty if table is not found\n",
    "    \n",
    "    # scrape the info, assign them to metric_name and value\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) == 2:\n",
    "            metric_name = cells[0].text.strip()\n",
    "            value = cells[1].text.strip()\n",
    "        elif len(cells) == 3:\n",
    "            # for when metric has two parts seperated by \"_\"\n",
    "            metric_name = f\"{cells[0].text.strip()}_{cells[1].text.strip()}\"\n",
    "            value = cells[2].text.strip()\n",
    "        else:\n",
    "            # print(\"else\")\n",
    "            continue\n",
    "        \n",
    "        # add everything to data dictionary\n",
    "        if metric_name in metrics:\n",
    "            try:\n",
    "                data[metric_name] = float(value)\n",
    "            except ValueError:\n",
    "                data[metric_name] = value\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Organize data into JSON\n",
    "# quality_data = {\"mriqc_failed\": {}, \"mriqc_passed\": {}}\n",
    "\n",
    "# Read metric names from CSV (BOLD_quality_metrics)\n",
    "metrics = set()\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        metric_name = row['Metric'] #changed this later, didn't check, might have problem!!!\n",
    "        metrics.add(metric_name)\n",
    "        \n",
    "# # Process both mriqc_failed_txt and mriqc_passed_txt files.\n",
    "# for category, txt_file in [(\"mriqc_failed\", mriqc_failed_txt), (\"mriqc_passed\", mriqc_passed_txt)]:\n",
    "#     with open(txt_file, \"r\") as f:\n",
    "\n",
    "#         # filenames is a list of all full html file names in failed and passed combined\n",
    "#         filenames = [line.strip() for line in f]\n",
    "\n",
    "#     # sets mriqc_dir as the current directory\n",
    "#     if category == \"mriqc_failed\": \n",
    "#         mriqc_dir = mriqc_failed_dir\n",
    "#     else:\n",
    "#         mriqc_dir = mriqc_passed_dir\n",
    "\n",
    "#     # for each html in mriqc_passed or mriqc_failed, get the metrics using extract_metrics function \n",
    "#     for filename in filenames:\n",
    "#         html_path = os.path.join(mriqc_dir, filename)\n",
    "#         if os.path.isfile(html_path):\n",
    "#             subject, task = parse_filename(filename)\n",
    "#             if task:\n",
    "#                 if task not in quality_data[category]:\n",
    "#                     quality_data[category][task] = {}\n",
    "#                 if subject not in quality_data[category][task]:\n",
    "#                     quality_data[category][task][subject] = {}\n",
    "#                 quality_data[category][task][subject] = extract_metrics(html_path, metrics)\n",
    "#         else:\n",
    "#             print(f\"not a path name {filename}\")\n",
    "    \n",
    "# # Save quality_data to json file.\n",
    "# with open(\"quality_metrics/quality_metrics_all.json\", \"w\") as f:\n",
    "#     json.dump(quality_data, f, indent=4)\n",
    "\n",
    "# print(\"JSON file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reformat_json(input_json, output_json):\n",
    "#     \"\"\"Reformats jason with a following hierarchy: \n",
    "#     from: category/task/subject/metric-value\n",
    "#     to: category/task/metric/subject-value\n",
    "#     creates output_json\n",
    "\n",
    "#     Args:\n",
    "#         input_json (_type_): _description_\n",
    "#         output_json (_type_): _description_\n",
    "#     \"\"\"\n",
    "#     with open(input_json, \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     reformatted = {\"mriqc_failed\": {}, \"mriqc_passed\": {}}\n",
    "    \n",
    "#     for category in [\"mriqc_failed\", \"mriqc_passed\"]:\n",
    "#         for task, subjects in data.get(category, {}).items():\n",
    "#             if task not in reformatted[category]:\n",
    "#                 reformatted[category][task] = {}\n",
    "            \n",
    "#             for subject, metrics in subjects.items():\n",
    "#                 for metric, value in metrics.items():\n",
    "#                     if metric not in reformatted[category][task]:\n",
    "#                         reformatted[category][task][metric] = {}\n",
    "                    \n",
    "#                     reformatted[category][task][metric][subject] = value\n",
    "    \n",
    "#     with open(output_json, \"w\") as f:\n",
    "#         json.dump(reformatted, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_json = \"quality_metrics/quality_metrics_all.json\"\n",
    "output_json = \"quality_metrics/quality_metrics_all_reformatted.json\"\n",
    "metrics_csv = \"quality_metrics/BOLD_quality_metrics.csv\"\n",
    "\n",
    "# Run functions\n",
    "# reformat_json(input_json, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves metrics for plot functions\n",
    "# dictionary saving low, median, and high values for each metric\n",
    "thresholds = dict()\n",
    "\n",
    "# opens metrics csv (listing names and thresholds)\n",
    "with open(metrics_csv, \"r\") as f:\n",
    "    reader = csv.DictReader(f)  # Read CSV as dictionary\n",
    "    for row in reader:\n",
    "        # metric = metric name\n",
    "        metric = row[\"Metric\"]  # Get the metric name\n",
    "\n",
    "        # thresholds = dictionary with keys: metric, values: low, high, and median\n",
    "        low, high, median = float(row[\"Low\"]), float(row[\"High\"]), float(row[\"Median\"])  # Convert to floats\n",
    "        thresholds[metric] = (low, high, median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TO DO'S IN THIS FUNCTION:\n",
    "# ADD MRIQC_FAILED FOR ABOVE THE LINE, PASSED FOR BELOW THE LINE\n",
    "# CLARIFY SHADED GRAY REGION (WITHIN RANGE OF LOW AND HIGH)\n",
    "# ADD LEGEND: RED DOT: OUTSIDE RANGE, GREEN DOT: IN RANGE\n",
    "\n",
    "# always display -1.5 to 1.5 in znormalized values\n",
    "def plot_task_metric(task, metric, data, thresholds, save_dir=\"quality_metrics/plots\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        task (_type_): _description_\n",
    "        metric (_type_): _description_\n",
    "        data (_type_): _description_\n",
    "        thresholds (_type_): _description_\n",
    "        save_dir (str, optional): _description_. Defaults to \"quality_metrics/plots\".\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    subjects = []\n",
    "    values = []\n",
    "    colors = []\n",
    "    categories = []\n",
    "    \n",
    "    # Extract data for both \"mriqc_failed\" and \"all_included\"\n",
    "    for category in [\"mriqc_passed\",\"mriqc_failed\"]:\n",
    "        if task in data[category] and metric in data[category][task]:\n",
    "            for subject, value in data[category][task][metric].items():\n",
    "                subjects.append(subject)\n",
    "                values.append(value)\n",
    "                categories.append(category)  # Keep track of whether it's \"mriqc_failed\" or \"all_included\"\n",
    "    \n",
    "    # Z-normalize values using Low, High, and Median from thresholds\n",
    "    if metric in thresholds:\n",
    "        (low, high, median) = thresholds[metric]\n",
    "        std_dev = (high - low) / 2  # Approximate standard deviation\n",
    "        values = [(v - median) / std_dev for v in values]\n",
    "\n",
    "    # Assign colors (red if outside range, green if inside)\n",
    "    for value in values:\n",
    "        if -1 <= value <= 1:  # Within expected range\n",
    "            colors.append(\"green\")\n",
    "        else:\n",
    "            colors.append(\"red\")\n",
    "    \n",
    "    # Separate \"mriqc_failed\" from \"mriqc_passed\" in the plot\n",
    "    passed_count = sum(1 for cat in categories if cat == \"mriqc_passed\")\n",
    "    \n",
    "    fig_height = max(6, len(subjects) * 0.3)  # Store the calculated height\n",
    "    # Increase figure height\n",
    "    fig, ax = plt.subplots(figsize=(8, fig_height))\n",
    "    \n",
    "    ax.scatter(values, range(len(subjects)), c=colors, s=50)\n",
    "    \n",
    "    # Add a horizontal separation line between failed and included\n",
    "    if passed_count > 0:\n",
    "        ax.axhline(y=passed_count - 0.5, color=\"black\", linestyle=\"--\")\n",
    "        if (len(values)-1 > passed_count):\n",
    "            if  values[passed_count] < 1.0:\n",
    "                ax.text(-1.47, passed_count + 0.5, \"mriqc_failed\", verticalalignment='top', fontsize=8)\n",
    "            else:\n",
    "                ax.text(1.0, passed_count + 0.5, \"mriqc_failed\", verticalalignment='top', fontsize=8)\n",
    "        \n",
    "        if values[passed_count-1]< 1.0:\n",
    "            ax.text(-1.47, passed_count - 1.5, \"mriqc_passed\", verticalalignment='bottom', fontsize=8)\n",
    "        else:\n",
    "            ax.text(1.0, passed_count - 1.5, \"mriqc_passed\", verticalalignment='bottom', fontsize=8)\n",
    "\n",
    "    # Set y-axis labels (subjects)\n",
    "    ax.set_yticks(range(len(subjects)))\n",
    "    ax.set_yticklabels(subjects)\n",
    "    \n",
    "    # Add plot title and labels\n",
    "    plt.title(\"Suggested Exclusions:\\n\" + f\"{task} - {metric} Plot\", fontsize=14, pad=20, y= 1)\n",
    "    plt.xlabel(\"Z-Normalized Value\", fontsize=12)\n",
    "    plt.ylabel(\"Subjects\", fontsize=12)\n",
    "    \n",
    "    # Draw shaded gray region for acceptable range (-1 to 1)\n",
    "    ax.axvspan(-1, 1, color=\"gray\", alpha=0.2)\n",
    "    \n",
    "    # Add small annotations for value ranges\n",
    "    if (values[0] > 0.0):\n",
    "        ax.text(-0.5, -0.4, \"within range\", ha=\"center\", fontsize=8)\n",
    "    else:\n",
    "        ax.text(0.5, -0.4, \"within range\", ha=\"center\", fontsize=8)\n",
    "    ax.text(-1.0, -0.4, \"low\\nthreshold\",  ha=\"center\", fontsize=8)\n",
    "    ax.text(1.0, -0.4, \"high\\nthreshold\",  ha=\"center\", fontsize=8)\n",
    "\n",
    "    # Ensure x-axis ticks are visible\n",
    "    # plt.xticks(range(int(min(values)) - 1, int(max(values)) + 2))\n",
    "    plt.xticks([-1.5, -1.0, -0.5, 0, 0.5, 1.0, 1.5])\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.scatter([], [], color=\"green\", label=\"In range\")\n",
    "    ax.scatter([], [], color=\"red\", label=\"Outside range\")\n",
    "\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    # Adjust margins\n",
    "    plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"{task}-{metric}.png\"), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the plot function in one file to make sure it's working correctly: \n",
    "with open(output_json, \"r\") as f:\n",
    "    data = json.load(f)  # Use json.load() instead of json.loads()\n",
    "    plot_task_metric(\"ANT\", \"dvars_vstd\", data, thresholds, save_dir=\"quality_metrics/plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all combinations of task-metric plots\n",
    "with open(output_json, \"r\") as f:\n",
    "    data = json.load(f) \n",
    "    for task in TASKS:\n",
    "        for metric in list(thresholds.keys()):\n",
    "\n",
    "            # for each unique task-metric combination, plot a graph \n",
    "            # data(json file), thresholds (dictionary)\n",
    "            plot_task_metric(task, metric, data, thresholds, save_dir=\"quality_metrics/plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot subject task\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_subject_task(category, task, subject, data, thresholds, save_dir):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        task (_type_): _description_\n",
    "        metric (_type_): _description_\n",
    "        data (_type_): _description_\n",
    "        thresholds (_type_): _description_\n",
    "        save_dir (str, optional): _description_. Defaults to \"quality_metrics/plots\".\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    metrics = []\n",
    "    values = []\n",
    "    colors = []\n",
    "    categories = []\n",
    "    \n",
    "\n",
    "    # Extract data for both \"mriqc_failed\" and \"all_included\"\n",
    "    # takes in quality_metrics_all (not reformatted)\n",
    "    category = category\n",
    "    if task in data[category] and subject in data[category][task]:\n",
    "        for metric, value in data[category][task][subject].items():\n",
    "            metrics.append(metric)\n",
    "            values.append(value)\n",
    "            categories.append(category)  # Keep track of whether it's \"mriqc_failed\" or \"all_included\"\n",
    "    \n",
    "    # Z-normalize values using Low, High, and Median from thresholds\n",
    "    # update the values array with normalized values.\n",
    "    for metric in metrics:\n",
    "        (low, high, median) = thresholds[metric]\n",
    "        std_dev = (high - low) / 2  # Approximate standard deviation\n",
    "        value_idx = metrics.index(metric)\n",
    "        values[value_idx] = (values[value_idx] - median) / std_dev\n",
    "\n",
    "    # Assign colors (red if outside range, green if inside)\n",
    "    for value in values:\n",
    "        if -1 <= value <= 1:  # Within expected range\n",
    "            colors.append(\"green\")\n",
    "        else:\n",
    "            colors.append(\"red\")\n",
    "    \n",
    "    \n",
    "    # Increase figure height\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.scatter(range(len(metrics)), values, c=colors, s=50)\n",
    "\n",
    "    # Set y-axis labels (subjects)\n",
    "    ax.set_yticks([-2, -1, 0, 1, 2])\n",
    "    \n",
    "    # Add plot title and labels\n",
    "    plt.title(\"Suggested Exclusions:\\n\" + f\"{subject} - {task} Plot - {category}\", fontsize=14, pad=20, y= 1)\n",
    "    plt.ylabel(\"Z-Normalized Value\", fontsize=12)\n",
    "    plt.xlabel(\"Metrics\", fontsize=12)\n",
    "    \n",
    "    # Draw shaded gray region for acceptable range (-1 to 1)\n",
    "    ax.axhspan(-1, 1, color=\"gray\", alpha=0.2)\n",
    "    \n",
    "    # Add small annotations for value ranges\n",
    "\n",
    "    if values[-1] < 0:\n",
    "        ax.text(4.8, 0.5, \"within\\nrange\", verticalalignment='center', ha=\"center\", fontsize=8)\n",
    "    else: \n",
    "        ax.text(4.8, -0.5, \"within\\nrange\", verticalalignment='center', ha=\"center\",fontsize=8)\n",
    "    ax.text(4.8, -1, \"low\\nthreshold\", verticalalignment='center', ha=\"center\",fontsize=8)\n",
    "    ax.text(4.8, 1, \"high\\nthreshold\", verticalalignment='center', ha=\"center\",fontsize=8)\n",
    "\n",
    "    # Ensure x-axis ticks are visible\n",
    "    # plt.xticks(range(int(min(values)) - 1, int(max(values)) + 2)\n",
    "    plt.xticks(range(6), metrics)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.scatter([], [], color=\"green\", label=\"In range\")\n",
    "    ax.scatter([], [], color=\"red\", label=\"Outside range\")\n",
    "\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.ylim(-2,2)\n",
    "    # Adjust margins\n",
    "    plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"{category}-{subject}-{task}.png\"), dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dvars_vstd', 'fber', 'fd_mean', 'gsr_y', 'snr', 'tsnr']\n"
     ]
    }
   ],
   "source": [
    "# try the plot function in one file to make sure it's working correctly: \n",
    "output_json = \"quality_metrics/quality_metrics_all.json\"\n",
    "with open(output_json, \"r\") as f:\n",
    "    data = json.load(f)  # Use json.load() instead of json.loads()\n",
    "    plot_subject_task(\"mriqc_failed\", \"CCTHot\", \"s607\", data, thresholds, save_dir=\"quality_metrics/plots/subject-task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all combinations of task-metric plots\n",
    "with open(output_json, \"r\") as f:\n",
    "    data = json.load(f) \n",
    "    for category,task in data.items(): \n",
    "        for task, subject in task.items():\n",
    "            for subject,metrics in subject.items():\n",
    "                # for each unique task-metric combination, plot a graph \n",
    "                # data(json file), thresholds (dictionary)\n",
    "                plot_subject_task(category, task, subject, data, thresholds, save_dir=\"quality_metrics/plots/subject-task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
